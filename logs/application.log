2024-03-30 01:39:06,077 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 01:39:06,114 - INFO - Last run was successful!!!
2024-03-30 01:39:07,079 - INFO - Total files available in folder 'sales_data/' of bucket 'manish-de-spark-project-1': {'ResponseMetadata': {'RequestId': 'YZX6TTRE2NAK09QC', 'HostId': 'Ae9SUpvCM91DvqJKl876GdBat3XR6OoSsiUQ4xNrPU4OPGrtRgZW49DO9pqoxyCqC1oTXD6JD9xpsA9eoucBdw==', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'Ae9SUpvCM91DvqJKl876GdBat3XR6OoSsiUQ4xNrPU4OPGrtRgZW49DO9pqoxyCqC1oTXD6JD9xpsA9eoucBdw==', 'x-amz-request-id': 'YZX6TTRE2NAK09QC', 'date': 'Sat, 30 Mar 2024 05:39:08 GMT', 'x-amz-bucket-region': 'us-east-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Contents': [{'Key': 'sales_data/extra_column_sales_data_2023_11_01.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 46, tzinfo=tzutc()), 'ETag': '"0cc6d8a83a66cbfb151e98af0f80af9e"', 'Size': 44125, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/less_column_sales_data_2023-10-17.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 46, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 46, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-13.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 46, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-15.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 46, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-17.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 46, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2024-01-03.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 30, 45, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}], 'Name': 'manish-de-spark-project-1', 'Prefix': 'sales_data/', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 7}
2024-03-30 01:39:07,080 - INFO - Absolute path on s3 bucket for csv file ['s3://manish-de-spark-project-1/sales_data/extra_column_sales_data_2023_11_01.csv', 's3://manish-de-spark-project-1/sales_data/less_column_sales_data_2023-10-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-13.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-15.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2024-01-03.csv'] 
2024-03-30 01:39:07,080 - INFO - File path available on S3 under manish-de-spark-project-1 bucket and file path is ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-03-30 01:39:07,080 - INFO - Running download files for these files ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-03-30 01:39:07,080 - INFO - File name extra_column_sales_data_2023_11_01.csv 
2024-03-30 01:39:07,258 - INFO - File name less_column_sales_data_2023-10-17.csv 
2024-03-30 01:39:07,422 - INFO - File name sales_data.csv 
2024-03-30 01:39:07,567 - INFO - File name sales_data_2023-12-13.csv 
2024-03-30 01:39:07,702 - INFO - File name sales_data_2023-12-15.csv 
2024-03-30 01:39:07,866 - INFO - File name sales_data_2023-12-17.csv 
2024-03-30 01:39:07,997 - INFO - File name sales_data_2024-01-03.csv 
2024-03-30 01:39:08,154 - INFO - list of files present in the local directory after download: ['sales_data_2024-01-03.csv', 'sales_data.csv', 'sales_data_2023-12-15.csv', 'less_column_sales_data_2023-10-17.csv', 'extra_column_sales_data_2023_11_01.csv', 'sales_data_2023-12-13.csv', 'sales_data_2023-12-17.csv']
2024-03-30 01:39:08,155 - INFO - **********************Listing the files***********************
2024-03-30 01:39:08,155 - INFO - listing the csv files that needs to be processed: ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-03-30 01:39:08,155 - INFO - ******************* Creating Spark Session *******************
2024-03-30 01:39:12,380 - INFO - spark session <pyspark.sql.session.SparkSession object at 0x78c502fb2bc0>
2024-03-30 01:39:12,380 - INFO - ******************* Spark Session Created *******************
2024-03-30 01:39:12,380 - INFO - Checking schema of data loaded in s3
2024-03-30 01:39:18,111 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:18,111 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:18,111 - INFO - Missing column(s) is/are: set()
2024-03-30 01:39:18,111 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 01:39:18,436 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:18,437 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:18,437 - INFO - Missing column(s) is/are: set()
2024-03-30 01:39:18,437 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 01:39:18,684 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:39:18,684 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:18,684 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:39:18,980 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:39:18,980 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:18,980 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:39:19,257 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:39:19,258 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:19,258 - INFO - Missing column(s) is/are: set()
2024-03-30 01:39:19,258 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 01:39:19,459 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:39:19,459 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:19,459 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:39:19,653 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:39:19,653 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:39:19,654 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:39:19,654 - INFO - ******************** List of correct files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv']
2024-03-30 01:39:19,654 - INFO - ******************** List of error files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-03-30 01:39:19,654 - INFO - ******************** Moving the error files to error directory if any********************
2024-03-30 01:39:19,654 - INFO - Moved sales_data_2023-12-15.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-15.csv.
2024-03-30 01:39:19,890 - INFO - Moved file: sales_data/sales_data_2023-12-15.csv to sales_data_error/sales_data_2023-12-15.csv
2024-03-30 01:39:19,890 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:39:19,891 - INFO - Moved less_column_sales_data_2023-10-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/less_column_sales_data_2023-10-17.csv.
2024-03-30 01:39:20,103 - INFO - Moved file: sales_data/less_column_sales_data_2023-10-17.csv to sales_data_error/less_column_sales_data_2023-10-17.csv
2024-03-30 01:39:20,103 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:39:20,104 - INFO - Moved sales_data_2023-12-13.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-13.csv.
2024-03-30 01:39:20,315 - INFO - Moved file: sales_data/sales_data_2023-12-13.csv to sales_data_error/sales_data_2023-12-13.csv
2024-03-30 01:39:20,315 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:39:20,316 - INFO - Moved sales_data_2023-12-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-17.csv.
2024-03-30 01:39:20,590 - INFO - Moved file: sales_data/sales_data_2023-12-17.csv to sales_data_error/sales_data_2023-12-17.csv
2024-03-30 01:39:20,591 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:39:20,591 - INFO - ******************************* Updating the product_staging_table to indicate process is started ************************************
2024-03-30 01:39:20,591 - INFO - Insert statement created for staging table ---- ["\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '2024-03-30 01.39.20', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '2024-03-30 01.39.20', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '2024-03-30 01.39.20', 'A')\n            "]
2024-03-30 01:39:20,592 - INFO - *************************** Connecting to MySql server ***************************
2024-03-30 01:39:20,637 - INFO - *************************** MySql connected Successfully ***************************
2024-03-30 01:39:20,656 - INFO - ********************************** Staging table updated successfully ***************************************
2024-03-30 01:39:20,656 - INFO - **************************** Fixing extra columns coming from source *****************************
2024-03-30 01:39:20,731 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 01:39:21,328 - INFO - Extra columns present at source. Extra columns: []
2024-03-30 01:39:21,416 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 01:39:21,709 - INFO - Extra columns present at source. Extra columns: []
2024-03-30 01:39:21,757 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 01:39:22,049 - INFO - Extra columns present at source. Extra columns: ['payment_mode']
2024-03-30 01:39:22,101 - INFO - processed /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv and added additional columns
2024-03-30 01:39:22,109 - INFO - ****************** Final dataframe from source which will be going for processing *****************
2024-03-30 01:39:23,916 - INFO - **************** Loading customer table into customer_table_df *********************
2024-03-30 01:39:24,390 - INFO - **************** Loading product table into product_table_df *********************
2024-03-30 01:39:24,438 - INFO - **************** Loading product staging table into product_staging_table_df *********************
2024-03-30 01:39:24,478 - INFO - **************** Loading sales team table into sales_team_table_df *********************
2024-03-30 01:39:24,524 - INFO - **************** Loading store table into store_table_df *********************
2024-03-30 01:39:24,565 - INFO - Joining the final_df_to_process with customer_table_df 
2024-03-30 01:39:24,644 - INFO - Joining the s3_customer_df_join with store_table_df 
2024-03-30 01:39:24,681 - INFO - Joining the s3_customer_store_df_join with sales_team_table_df 
2024-03-30 01:39:24,796 - INFO - *********************** Final enriched data *****************************
2024-03-30 01:39:27,334 - INFO - ********************** Write the data into customer data mart ******************************
2024-03-30 01:39:27,379 - INFO - ************** Final Data for Customer Data Mart ********************
2024-03-30 01:39:30,704 - INFO - *************** customer data written to local disk at /home/hdoop/projects/project-1/spark_data/customer_data_mart/ **********
2024-03-30 01:39:30,704 - INFO - ************ Data Movement from local to S3 for customer data mart **************
2024-03-30 01:39:31,156 - INFO - Data Successfully uploaded in customer_data_mart data mart 
2024-03-30 01:39:31,156 - INFO - **************** Write the data into sales team data mart *******************
2024-03-30 01:39:31,318 - INFO - ********************** Final Data for Sales Team Data Mart ************************
2024-03-30 01:39:33,569 - INFO - ********** sales team data written to local disk at /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/ **********
2024-03-30 01:39:34,025 - INFO - Data Successfully uploaded in sales_data_mart data mart 
2024-03-30 01:39:41,641 - INFO - ******************* Calculating customer every month purchased amount **********************
2024-03-30 01:39:44,862 - INFO - Data successfully written into customers_data_mart table 
2024-03-30 01:39:44,862 - INFO - ******************* Calculation of customer mart done and written into the tables **********************
2024-03-30 01:39:44,862 - INFO - Calculating sales monthly billed amount
2024-03-30 01:39:46,567 - INFO - Data successfully written into sales_team_data_mart table 
2024-03-30 01:39:46,568 - INFO - Calculation of sales mart done and written into the table: sales_team_data_mart
2024-03-30 01:39:47,145 - INFO - Data Moved successfully from sales_data/ to sales_data_processed/
2024-03-30 01:39:47,146 - INFO - ******************* Deleting sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-03-30 01:39:47,147 - INFO - ******************* Deleted sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-03-30 01:39:47,147 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-03-30 01:39:47,148 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-03-30 01:39:47,148 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-03-30 01:39:47,148 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-03-30 01:39:47,148 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-03-30 01:39:47,158 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-03-30 01:39:47,158 - INFO - Update statement created for staging table: ["UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 01.39.20' WHERE file_name = 'sales_data_2024-01-03.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 01.39.20' WHERE file_name = 'sales_data.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 01.39.20' WHERE file_name = 'extra_column_sales_data_2023_11_01.csv'"]
2024-03-30 01:39:47,158 - INFO - **************** Connecting to MySQL server *******************
2024-03-30 01:39:47,188 - INFO - *********** MySQL connected successfully **************
2024-03-30 01:42:07,711 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 01:42:07,747 - INFO - Last run was successful!!!
2024-03-30 01:42:08,894 - INFO - Total files available in folder 'sales_data/' of bucket 'manish-de-spark-project-1': {'ResponseMetadata': {'RequestId': '9C655HR1FBQ9AP66', 'HostId': 'PMoyeidcM/fJBfMjWZJtPJLrMNV6qrZwn+W9d+glaHYV28eF8H4BRtXN8tdj2Ct6qIwGb184IgDerj5GFUqwaA==', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'PMoyeidcM/fJBfMjWZJtPJLrMNV6qrZwn+W9d+glaHYV28eF8H4BRtXN8tdj2Ct6qIwGb184IgDerj5GFUqwaA==', 'x-amz-request-id': '9C655HR1FBQ9AP66', 'date': 'Sat, 30 Mar 2024 05:42:09 GMT', 'x-amz-bucket-region': 'us-east-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Contents': [{'Key': 'sales_data/extra_column_sales_data_2023_11_01.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 29, tzinfo=tzutc()), 'ETag': '"0cc6d8a83a66cbfb151e98af0f80af9e"', 'Size': 44125, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/less_column_sales_data_2023-10-17.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 29, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 29, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-13.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 30, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-15.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 29, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-17.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 30, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2024-01-03.csv', 'LastModified': datetime.datetime(2024, 3, 30, 5, 41, 29, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}], 'Name': 'manish-de-spark-project-1', 'Prefix': 'sales_data/', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 7}
2024-03-30 01:42:08,894 - INFO - Absolute path on s3 bucket for csv file ['s3://manish-de-spark-project-1/sales_data/extra_column_sales_data_2023_11_01.csv', 's3://manish-de-spark-project-1/sales_data/less_column_sales_data_2023-10-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-13.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-15.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2024-01-03.csv'] 
2024-03-30 01:42:08,894 - INFO - File path available on S3 under manish-de-spark-project-1 bucket and file path is ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-03-30 01:42:08,894 - INFO - Running download files for these files ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-03-30 01:42:08,895 - INFO - File name extra_column_sales_data_2023_11_01.csv 
2024-03-30 01:42:09,099 - INFO - File name less_column_sales_data_2023-10-17.csv 
2024-03-30 01:42:09,245 - INFO - File name sales_data.csv 
2024-03-30 01:42:09,390 - INFO - File name sales_data_2023-12-13.csv 
2024-03-30 01:42:09,537 - INFO - File name sales_data_2023-12-15.csv 
2024-03-30 01:42:09,698 - INFO - File name sales_data_2023-12-17.csv 
2024-03-30 01:42:09,839 - INFO - File name sales_data_2024-01-03.csv 
2024-03-30 01:42:09,979 - INFO - list of files present in the local directory after download: ['sales_data_2024-01-03.csv', 'sales_data.csv', 'sales_data_2023-12-15.csv', 'less_column_sales_data_2023-10-17.csv', 'extra_column_sales_data_2023_11_01.csv', 'sales_data_2023-12-13.csv', 'sales_data_2023-12-17.csv']
2024-03-30 01:42:09,980 - INFO - **********************Listing the files***********************
2024-03-30 01:42:09,980 - INFO - listing the csv files that needs to be processed: ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-03-30 01:42:09,980 - INFO - ******************* Creating Spark Session *******************
2024-03-30 01:42:11,858 - INFO - spark session <pyspark.sql.session.SparkSession object at 0x7db597582a40>
2024-03-30 01:42:11,858 - INFO - ******************* Spark Session Created *******************
2024-03-30 01:42:11,858 - INFO - Checking schema of data loaded in s3
2024-03-30 01:42:17,601 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:17,601 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:17,601 - INFO - Missing column(s) is/are: set()
2024-03-30 01:42:17,602 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 01:42:17,910 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:17,910 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:17,910 - INFO - Missing column(s) is/are: set()
2024-03-30 01:42:17,910 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 01:42:18,231 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:42:18,231 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:18,231 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:42:18,540 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:42:18,541 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:18,541 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:42:18,785 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:42:18,785 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:18,785 - INFO - Missing column(s) is/are: set()
2024-03-30 01:42:18,785 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 01:42:19,032 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:42:19,032 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:19,032 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:42:19,342 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 01:42:19,342 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 01:42:19,342 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 01:42:19,343 - INFO - ******************** List of correct files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv']
2024-03-30 01:42:19,343 - INFO - ******************** List of error files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-03-30 01:42:19,343 - INFO - ******************** Moving the error files to error directory if any********************
2024-03-30 01:42:19,343 - INFO - Moved sales_data_2023-12-15.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-15.csv.
2024-03-30 01:42:19,556 - INFO - Moved file: sales_data/sales_data_2023-12-15.csv to sales_data_error/sales_data_2023-12-15.csv
2024-03-30 01:42:19,556 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:42:19,556 - INFO - Moved less_column_sales_data_2023-10-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/less_column_sales_data_2023-10-17.csv.
2024-03-30 01:42:19,830 - INFO - Moved file: sales_data/less_column_sales_data_2023-10-17.csv to sales_data_error/less_column_sales_data_2023-10-17.csv
2024-03-30 01:42:19,830 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:42:19,831 - INFO - Moved sales_data_2023-12-13.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-13.csv.
2024-03-30 01:42:20,053 - INFO - Moved file: sales_data/sales_data_2023-12-13.csv to sales_data_error/sales_data_2023-12-13.csv
2024-03-30 01:42:20,054 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:42:20,054 - INFO - Moved sales_data_2023-12-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-17.csv.
2024-03-30 01:42:20,260 - INFO - Moved file: sales_data/sales_data_2023-12-17.csv to sales_data_error/sales_data_2023-12-17.csv
2024-03-30 01:42:20,261 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 01:42:20,261 - INFO - ******************************* Updating the product_staging_table to indicate process is started ************************************
2024-03-30 01:42:20,261 - INFO - Insert statement created for staging table ---- ["\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '2024-03-30 01.42.20', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '2024-03-30 01.42.20', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '2024-03-30 01.42.20', 'A')\n            "]
2024-03-30 01:42:20,262 - INFO - *************************** Connecting to MySql server ***************************
2024-03-30 01:42:20,302 - INFO - *************************** MySql connected Successfully ***************************
2024-03-30 01:42:20,327 - INFO - ********************************** Staging table updated successfully ***************************************
2024-03-30 01:42:20,328 - INFO - **************************** Fixing extra columns coming from source *****************************
2024-03-30 01:42:20,402 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 01:42:20,912 - INFO - Extra columns present at source. Extra columns: []
2024-03-30 01:42:21,013 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 01:42:21,405 - INFO - Extra columns present at source. Extra columns: []
2024-03-30 01:42:21,489 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 01:42:21,824 - INFO - Extra columns present at source. Extra columns: ['payment_mode']
2024-03-30 01:42:21,885 - INFO - processed /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv and added additional columns
2024-03-30 01:42:21,895 - INFO - ****************** Final dataframe from source which will be going for processing *****************
2024-03-30 01:42:23,675 - INFO - **************** Loading customer table into customer_table_df *********************
2024-03-30 01:42:24,284 - INFO - **************** Loading product table into product_table_df *********************
2024-03-30 01:42:24,327 - INFO - **************** Loading product staging table into product_staging_table_df *********************
2024-03-30 01:42:24,371 - INFO - **************** Loading sales team table into sales_team_table_df *********************
2024-03-30 01:42:24,424 - INFO - **************** Loading store table into store_table_df *********************
2024-03-30 01:42:24,465 - INFO - Joining the final_df_to_process with customer_table_df 
2024-03-30 01:42:24,527 - INFO - Joining the s3_customer_df_join with store_table_df 
2024-03-30 01:42:24,562 - INFO - Joining the s3_customer_store_df_join with sales_team_table_df 
2024-03-30 01:42:24,678 - INFO - *********************** Final enriched data *****************************
2024-03-30 01:42:27,591 - INFO - ********************** Write the data into customer data mart ******************************
2024-03-30 01:42:27,668 - INFO - ************** Final Data for Customer Data Mart ********************
2024-03-30 01:42:33,871 - INFO - *************** customer data written to local disk at /home/hdoop/projects/project-1/spark_data/customer_data_mart/ **********
2024-03-30 01:42:33,871 - INFO - ************ Data Movement from local to S3 for customer data mart **************
2024-03-30 01:42:34,337 - INFO - Data Successfully uploaded in customer_data_mart data mart 
2024-03-30 01:42:34,337 - INFO - **************** Write the data into sales team data mart *******************
2024-03-30 01:42:34,584 - INFO - ********************** Final Data for Sales Team Data Mart ************************
2024-03-30 01:42:37,518 - INFO - ********** sales team data written to local disk at /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/ **********
2024-03-30 01:42:38,037 - INFO - Data Successfully uploaded in sales_data_mart data mart 
2024-03-30 01:42:46,132 - INFO - ******************* Calculating customer every month purchased amount **********************
2024-03-30 01:42:50,058 - INFO - Data successfully written into customers_data_mart table 
2024-03-30 01:42:50,058 - INFO - ******************* Calculation of customer mart done and written into the tables **********************
2024-03-30 01:42:50,059 - INFO - Calculating sales monthly billed amount
2024-03-30 01:42:52,002 - INFO - Data successfully written into sales_team_data_mart table 
2024-03-30 01:42:52,002 - INFO - Calculation of sales mart done and written into the table: sales_team_data_mart
2024-03-30 01:42:52,542 - INFO - Data Moved successfully from sales_data/ to sales_data_processed/
2024-03-30 01:42:52,543 - INFO - ******************* Deleting sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-03-30 01:42:52,544 - INFO - ******************* Deleted sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-03-30 01:42:52,544 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-03-30 01:42:52,546 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-03-30 01:42:52,546 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-03-30 01:42:52,547 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-03-30 01:42:52,547 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-03-30 01:42:52,559 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-03-30 01:42:52,559 - INFO - Update statement created for staging table: ["UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 01.42.20' WHERE file_name = 'sales_data_2024-01-03.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 01.42.20' WHERE file_name = 'sales_data.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 01.42.20' WHERE file_name = 'extra_column_sales_data_2023_11_01.csv'"]
2024-03-30 01:42:52,560 - INFO - **************** Connecting to MySQL server *******************
2024-03-30 01:42:52,594 - INFO - *********** MySQL connected successfully **************
2024-03-30 02:53:44,625 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 02:59:58,862 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 03:11:18,668 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 03:15:46,218 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 03:18:13,591 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 03:44:44,408 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 03:44:44,410 - INFO - Last run was successful!!!
2024-03-30 03:44:45,215 - INFO - Absolute path on s3 bucket for csv file [] 
2024-03-30 03:44:45,216 - WARNING - No file available at sales_data/ in S3.
2024-03-30 03:44:45,216 - ERROR - Exited with error code: No data available in S3 to process.
2024-03-30 03:46:37,423 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-03-30 03:46:37,425 - INFO - Last run was successful!!!
2024-03-30 03:46:38,403 - INFO - Total files available in folder 'sales_data/' of bucket 'manish-de-spark-project-1': {'ResponseMetadata': {'RequestId': 'M5PR1BW20MVA7PJ7', 'HostId': 'rmtRj1C/AMEn6xPD4BZsKMB79NpD7KRiwbp1wvRvhaxB1HtIUqhfcGAhe8EpgXu1KcBX8PtUFYpgJCAxhAE2++WUsRO5sEaIgkCg0diMNf8=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'rmtRj1C/AMEn6xPD4BZsKMB79NpD7KRiwbp1wvRvhaxB1HtIUqhfcGAhe8EpgXu1KcBX8PtUFYpgJCAxhAE2++WUsRO5sEaIgkCg0diMNf8=', 'x-amz-request-id': 'M5PR1BW20MVA7PJ7', 'date': 'Sat, 30 Mar 2024 07:46:39 GMT', 'x-amz-bucket-region': 'us-east-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Contents': [{'Key': 'sales_data/extra_column_sales_data_2023_11_01.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 19, tzinfo=tzutc()), 'ETag': '"0cc6d8a83a66cbfb151e98af0f80af9e"', 'Size': 44125, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/less_column_sales_data_2023-10-17.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 19, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 18, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-13.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 19, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-15.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 18, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-17.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 19, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2024-01-03.csv', 'LastModified': datetime.datetime(2024, 3, 30, 7, 45, 18, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}], 'Name': 'manish-de-spark-project-1', 'Prefix': 'sales_data/', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 7}
2024-03-30 03:46:38,403 - INFO - Absolute path on s3 bucket for csv file ['s3://manish-de-spark-project-1/sales_data/extra_column_sales_data_2023_11_01.csv', 's3://manish-de-spark-project-1/sales_data/less_column_sales_data_2023-10-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-13.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-15.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2024-01-03.csv'] 
2024-03-30 03:46:38,404 - INFO - File path available on S3 under manish-de-spark-project-1 bucket and file path is ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-03-30 03:46:38,404 - INFO - Running download files for these files ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-03-30 03:46:38,404 - INFO - File name extra_column_sales_data_2023_11_01.csv 
2024-03-30 03:46:38,632 - INFO - File name less_column_sales_data_2023-10-17.csv 
2024-03-30 03:46:38,778 - INFO - File name sales_data.csv 
2024-03-30 03:46:38,908 - INFO - File name sales_data_2023-12-13.csv 
2024-03-30 03:46:39,043 - INFO - File name sales_data_2023-12-15.csv 
2024-03-30 03:46:39,173 - INFO - File name sales_data_2023-12-17.csv 
2024-03-30 03:46:39,364 - INFO - File name sales_data_2024-01-03.csv 
2024-03-30 03:46:39,501 - INFO - list of files present in the local directory after download: ['sales_data_2024-01-03.csv', 'sales_data.csv', 'sales_data_2023-12-15.csv', 'less_column_sales_data_2023-10-17.csv', 'extra_column_sales_data_2023_11_01.csv', 'sales_data_2023-12-13.csv', 'sales_data_2023-12-17.csv']
2024-03-30 03:46:39,501 - INFO - **********************Listing the files***********************
2024-03-30 03:46:39,502 - INFO - listing the csv files that needs to be processed: ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-03-30 03:46:39,502 - INFO - ******************* Creating Spark Session *******************
2024-03-30 03:46:43,667 - INFO - spark session <pyspark.sql.session.SparkSession object at 0x7ccbe1c78e80>
2024-03-30 03:46:43,668 - INFO - ******************* Spark Session Created *******************
2024-03-30 03:46:43,668 - INFO - Checking schema of data loaded in s3
2024-03-30 03:46:49,143 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:49,143 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:49,143 - INFO - Missing column(s) is/are: set()
2024-03-30 03:46:49,143 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 03:46:49,549 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:49,549 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:49,549 - INFO - Missing column(s) is/are: set()
2024-03-30 03:46:49,550 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 03:46:49,830 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 03:46:49,830 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:49,830 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 03:46:50,057 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 03:46:50,057 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:50,057 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 03:46:50,247 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 03:46:50,247 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:50,247 - INFO - Missing column(s) is/are: set()
2024-03-30 03:46:50,247 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 03:46:50,527 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 03:46:50,527 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:50,527 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 03:46:50,766 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-03-30 03:46:50,766 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-03-30 03:46:50,766 - INFO - Missing column(s) is/are: {'store_id'}
2024-03-30 03:46:50,766 - INFO - ******************** List of correct files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv']
2024-03-30 03:46:50,766 - INFO - ******************** List of error files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-03-30 03:46:50,766 - INFO - ******************** Moving the error files to error directory if any********************
2024-03-30 03:46:50,767 - INFO - Moved sales_data_2023-12-15.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-15.csv.
2024-03-30 03:46:51,135 - INFO - Moved file: sales_data/sales_data_2023-12-15.csv to sales_data_error/sales_data_2023-12-15.csv
2024-03-30 03:46:51,136 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 03:46:51,136 - INFO - Moved less_column_sales_data_2023-10-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/less_column_sales_data_2023-10-17.csv.
2024-03-30 03:46:51,358 - INFO - Moved file: sales_data/less_column_sales_data_2023-10-17.csv to sales_data_error/less_column_sales_data_2023-10-17.csv
2024-03-30 03:46:51,359 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 03:46:51,360 - INFO - Moved sales_data_2023-12-13.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-13.csv.
2024-03-30 03:46:51,603 - INFO - Moved file: sales_data/sales_data_2023-12-13.csv to sales_data_error/sales_data_2023-12-13.csv
2024-03-30 03:46:51,603 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 03:46:51,604 - INFO - Moved sales_data_2023-12-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-17.csv.
2024-03-30 03:46:51,852 - INFO - Moved file: sales_data/sales_data_2023-12-17.csv to sales_data_error/sales_data_2023-12-17.csv
2024-03-30 03:46:51,853 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-03-30 03:46:51,853 - INFO - ******************************* Updating the product_staging_table to indicate process is started ************************************
2024-03-30 03:46:51,853 - INFO - Insert statement created for staging table ---- ["\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '2024-03-30 03.46.51', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '2024-03-30 03.46.51', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '2024-03-30 03.46.51', 'A')\n            "]
2024-03-30 03:46:51,853 - INFO - *************************** Connecting to MySql server ***************************
2024-03-30 03:46:51,856 - INFO - *************************** MySql connected Successfully ***************************
2024-03-30 03:46:51,881 - INFO - ********************************** Staging table updated successfully ***************************************
2024-03-30 03:46:51,881 - INFO - **************************** Fixing extra columns coming from source *****************************
2024-03-30 03:46:51,964 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 03:46:52,458 - INFO - Extra columns present at source. Extra columns: []
2024-03-30 03:46:52,594 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 03:46:52,879 - INFO - Extra columns present at source. Extra columns: []
2024-03-30 03:46:52,932 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 03:46:53,225 - INFO - Extra columns present at source. Extra columns: ['payment_mode']
2024-03-30 03:46:53,306 - INFO - processed /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv and added additional columns
2024-03-30 03:46:53,315 - INFO - ****************** Final dataframe from source which will be going for processing *****************
2024-03-30 03:46:55,002 - INFO - **************** Loading customer table into customer_table_df *********************
2024-03-30 03:46:55,561 - INFO - **************** Loading product table into product_table_df *********************
2024-03-30 03:46:55,606 - INFO - **************** Loading product staging table into product_staging_table_df *********************
2024-03-30 03:46:55,647 - INFO - **************** Loading sales team table into sales_team_table_df *********************
2024-03-30 03:46:55,688 - INFO - **************** Loading store table into store_table_df *********************
2024-03-30 03:46:55,721 - INFO - Joining the final_df_to_process with customer_table_df 
2024-03-30 03:46:55,779 - INFO - Joining the s3_customer_df_join with store_table_df 
2024-03-30 03:46:55,818 - INFO - Joining the s3_customer_store_df_join with sales_team_table_df 
2024-03-30 03:46:55,928 - INFO - *********************** Final enriched data *****************************
2024-03-30 03:46:58,647 - INFO - ********************** Write the data into customer data mart ******************************
2024-03-30 03:46:58,679 - INFO - ************** Final Data for Customer Data Mart ********************
2024-03-30 03:47:02,131 - INFO - *************** customer data written to local disk at /home/hdoop/projects/project-1/spark_data/customer_data_mart/ **********
2024-03-30 03:47:02,131 - INFO - ************ Data Movement from local to S3 for customer data mart **************
2024-03-30 03:47:02,623 - INFO - Data Successfully uploaded in customer_data_mart data mart 
2024-03-30 03:47:02,624 - INFO - **************** Write the data into sales team data mart *******************
2024-03-30 03:47:02,800 - INFO - ********************** Final Data for Sales Team Data Mart ************************
2024-03-30 03:47:05,226 - INFO - ********** sales team data written to local disk at /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/ **********
2024-03-30 03:47:05,677 - INFO - Data Successfully uploaded in sales_data_mart data mart 
2024-03-30 03:47:13,242 - INFO - ******************* Calculating customer every month purchased amount **********************
2024-03-30 03:47:16,559 - INFO - Data successfully written into customers_data_mart table 
2024-03-30 03:47:16,559 - INFO - ******************* Calculation of customer mart done and written into the tables **********************
2024-03-30 03:47:16,559 - INFO - Calculating sales monthly billed amount
2024-03-30 03:47:18,311 - INFO - Data successfully written into sales_team_data_mart table 
2024-03-30 03:47:18,311 - INFO - Calculation of sales mart done and written into the table: sales_team_data_mart
2024-03-30 03:47:18,878 - INFO - Data Moved successfully from sales_data/ to sales_data_processed/
2024-03-30 03:47:18,879 - INFO - ******************* Deleting sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-03-30 03:47:18,879 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-03-30 03:47:18,880 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-03-30 03:47:18,880 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-03-30 03:47:18,880 - INFO - ******************* Deleted sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-03-30 03:47:18,880 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-03-30 03:47:18,880 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/_SUCCESS
2024-03-30 03:47:18,881 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/part-00000-c932d4e9-5c2a-4e2a-a839-c3fb7ecabbc8-c000.snappy.parquet
2024-03-30 03:47:18,881 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/._SUCCESS.crc
2024-03-30 03:47:18,881 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/.part-00000-c932d4e9-5c2a-4e2a-a839-c3fb7ecabbc8-c000.snappy.parquet.crc
2024-03-30 03:47:18,881 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-03-30 03:47:18,881 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-03-30 03:47:18,882 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/part-00000-64acc1ad-6fcd-4008-96ce-c4cd63c3f123-c000.snappy.parquet
2024-03-30 03:47:18,882 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/_SUCCESS
2024-03-30 03:47:18,882 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/._SUCCESS.crc
2024-03-30 03:47:18,882 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/.part-00000-64acc1ad-6fcd-4008-96ce-c4cd63c3f123-c000.snappy.parquet.crc
2024-03-30 03:47:18,883 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-03-30 03:47:18,883 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-03-30 03:47:18,884 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-05
2024-03-30 03:47:18,885 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-06
2024-03-30 03:47:18,887 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-07
2024-03-30 03:47:18,888 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-03
2024-03-30 03:47:18,888 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_partition_data/_SUCCESS
2024-03-30 03:47:18,889 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-11
2024-03-30 03:47:18,891 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-08
2024-03-30 03:47:18,892 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-04
2024-03-30 03:47:18,893 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_partition_data/._SUCCESS.crc
2024-03-30 03:47:18,893 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-03-30 03:47:18,893 - INFO - Update statement created for staging table: ["UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 03.46.51' WHERE file_name = 'sales_data_2024-01-03.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 03.46.51' WHERE file_name = 'sales_data.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-03-30 03.46.51' WHERE file_name = 'extra_column_sales_data_2023_11_01.csv'"]
2024-03-30 03:47:18,893 - INFO - **************** Connecting to MySQL server *******************
2024-03-30 03:47:18,894 - INFO - *********** MySQL connected successfully **************
2024-04-05 13:48:10,191 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-04-05 13:48:10,196 - INFO - Last run was successful!!!
2024-04-05 13:48:10,849 - INFO - Absolute path on s3 bucket for csv file [] 
2024-04-05 13:48:10,850 - WARNING - No file available at sales_data/ in S3.
2024-04-05 13:48:10,850 - ERROR - Exited with error code: No data available in S3 to process.
2024-04-05 13:49:03,547 - INFO - List of Buckets: [{'Name': 'manish-de-spark-project-1', 'CreationDate': datetime.datetime(2024, 3, 15, 15, 34, 33, tzinfo=tzutc())}]
2024-04-05 13:49:03,548 - INFO - Last run was successful!!!
2024-04-05 13:49:04,367 - INFO - Total files available in folder 'sales_data/' of bucket 'manish-de-spark-project-1': {'ResponseMetadata': {'RequestId': '8P00R5GQV4A2DDNJ', 'HostId': '1a76hxrQtetg8g9hVTuiDYmJPBgffGht99lPzkXyauw4z0ih86J4pBuiY/AY3FzANdM7CrFLMmLY0CRc4mhqeA==', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': '1a76hxrQtetg8g9hVTuiDYmJPBgffGht99lPzkXyauw4z0ih86J4pBuiY/AY3FzANdM7CrFLMmLY0CRc4mhqeA==', 'x-amz-request-id': '8P00R5GQV4A2DDNJ', 'date': 'Fri, 05 Apr 2024 17:49:05 GMT', 'x-amz-bucket-region': 'us-east-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Contents': [{'Key': 'sales_data/extra_column_sales_data_2023_11_01.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 52, tzinfo=tzutc()), 'ETag': '"0cc6d8a83a66cbfb151e98af0f80af9e"', 'Size': 44125, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/less_column_sales_data_2023-10-17.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 52, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 52, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-13.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 53, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-15.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 52, tzinfo=tzutc()), 'ETag': '"99cc94d38e155721ebc5e973b75b9172"', 'Size': 9988, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2023-12-17.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 53, tzinfo=tzutc()), 'ETag': '"bda03cec01d8062894990369c9ebad34"', 'Size': 10029, 'StorageClass': 'STANDARD'}, {'Key': 'sales_data/sales_data_2024-01-03.csv', 'LastModified': datetime.datetime(2024, 4, 5, 17, 48, 52, tzinfo=tzutc()), 'ETag': '"5cfb2bff8f0082fca1ae8b0fce80dcef"', 'Size': 19717, 'StorageClass': 'STANDARD'}], 'Name': 'manish-de-spark-project-1', 'Prefix': 'sales_data/', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 7}
2024-04-05 13:49:04,368 - INFO - Absolute path on s3 bucket for csv file ['s3://manish-de-spark-project-1/sales_data/extra_column_sales_data_2023_11_01.csv', 's3://manish-de-spark-project-1/sales_data/less_column_sales_data_2023-10-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-13.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-15.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2023-12-17.csv', 's3://manish-de-spark-project-1/sales_data/sales_data_2024-01-03.csv'] 
2024-04-05 13:49:04,368 - INFO - File path available on S3 under manish-de-spark-project-1 bucket and file path is ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-04-05 13:49:04,368 - INFO - Running download files for these files ['sales_data/extra_column_sales_data_2023_11_01.csv', 'sales_data/less_column_sales_data_2023-10-17.csv', 'sales_data/sales_data.csv', 'sales_data/sales_data_2023-12-13.csv', 'sales_data/sales_data_2023-12-15.csv', 'sales_data/sales_data_2023-12-17.csv', 'sales_data/sales_data_2024-01-03.csv']
2024-04-05 13:49:04,369 - INFO - File name extra_column_sales_data_2023_11_01.csv 
2024-04-05 13:49:04,555 - INFO - File name less_column_sales_data_2023-10-17.csv 
2024-04-05 13:49:04,729 - INFO - File name sales_data.csv 
2024-04-05 13:49:04,881 - INFO - File name sales_data_2023-12-13.csv 
2024-04-05 13:49:05,026 - INFO - File name sales_data_2023-12-15.csv 
2024-04-05 13:49:05,161 - INFO - File name sales_data_2023-12-17.csv 
2024-04-05 13:49:05,323 - INFO - File name sales_data_2024-01-03.csv 
2024-04-05 13:49:05,459 - INFO - list of files present in the local directory after download: ['sales_data_2024-01-03.csv', 'sales_data.csv', 'sales_data_2023-12-15.csv', 'less_column_sales_data_2023-10-17.csv', 'extra_column_sales_data_2023_11_01.csv', 'sales_data_2023-12-13.csv', 'sales_data_2023-12-17.csv']
2024-04-05 13:49:05,459 - INFO - **********************Listing the files***********************
2024-04-05 13:49:05,460 - INFO - listing the csv files that needs to be processed: ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-04-05 13:49:05,460 - INFO - ******************* Creating Spark Session *******************
2024-04-05 13:49:11,313 - INFO - spark session <pyspark.sql.session.SparkSession object at 0x700964638f70>
2024-04-05 13:49:11,313 - INFO - ******************* Spark Session Created *******************
2024-04-05 13:49:11,313 - INFO - Checking schema of data loaded in s3
2024-04-05 13:49:17,470 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:17,470 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:17,470 - INFO - Missing column(s) is/are: set()
2024-04-05 13:49:17,470 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-04-05 13:49:17,865 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:17,866 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:17,866 - INFO - Missing column(s) is/are: set()
2024-04-05 13:49:17,866 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-04-05 13:49:18,166 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-04-05 13:49:18,167 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:18,167 - INFO - Missing column(s) is/are: {'store_id'}
2024-04-05 13:49:18,429 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-04-05 13:49:18,429 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:18,430 - INFO - Missing column(s) is/are: {'store_id'}
2024-04-05 13:49:18,748 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv are ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-04-05 13:49:18,748 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:18,748 - INFO - Missing column(s) is/are: set()
2024-04-05 13:49:18,748 - INFO - No missing column for the /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-04-05 13:49:19,051 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-04-05 13:49:19,051 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:19,051 - INFO - Missing column(s) is/are: {'store_id'}
2024-04-05 13:49:19,247 - INFO - Schema/Columns in /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv are ['customer_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost', 'payment_mode']
2024-04-05 13:49:19,247 - INFO - Mandatory columns in schema is: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']
2024-04-05 13:49:19,247 - INFO - Missing column(s) is/are: {'store_id'}
2024-04-05 13:49:19,247 - INFO - ******************** List of correct files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv']
2024-04-05 13:49:19,247 - INFO - ******************** List of error files******************** ['/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv']
2024-04-05 13:49:19,247 - INFO - ******************** Moving the error files to error directory if any********************
2024-04-05 13:49:19,248 - INFO - Moved sales_data_2023-12-15.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-15.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-15.csv.
2024-04-05 13:49:19,485 - INFO - Moved file: sales_data/sales_data_2023-12-15.csv to sales_data_error/sales_data_2023-12-15.csv
2024-04-05 13:49:19,485 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-04-05 13:49:19,486 - INFO - Moved less_column_sales_data_2023-10-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/less_column_sales_data_2023-10-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/less_column_sales_data_2023-10-17.csv.
2024-04-05 13:49:19,733 - INFO - Moved file: sales_data/less_column_sales_data_2023-10-17.csv to sales_data_error/less_column_sales_data_2023-10-17.csv
2024-04-05 13:49:19,733 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-04-05 13:49:19,734 - INFO - Moved sales_data_2023-12-13.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-13.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-13.csv.
2024-04-05 13:49:19,945 - INFO - Moved file: sales_data/sales_data_2023-12-13.csv to sales_data_error/sales_data_2023-12-13.csv
2024-04-05 13:49:19,945 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-04-05 13:49:19,946 - INFO - Moved sales_data_2023-12-17.csv file from /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2023-12-17.csv path to /home/hdoop/projects/project-1/spark_data/error_files/sales_data_2023-12-17.csv.
2024-04-05 13:49:20,168 - INFO - Moved file: sales_data/sales_data_2023-12-17.csv to sales_data_error/sales_data_2023-12-17.csv
2024-04-05 13:49:20,168 - INFO - Data Moved successfully from sales_data/ to sales_data_error/
2024-04-05 13:49:20,169 - INFO - ******************************* Updating the product_staging_table to indicate process is started ************************************
2024-04-05 13:49:20,169 - INFO - Insert statement created for staging table ---- ["\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data_2024-01-03.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv', '2024-04-05 13.49.20', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('sales_data.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv', '2024-04-05 13.49.20', 'A')\n            ", "\n            INSERT INTO manish_spark_de.product_staging_table \n            (file_name, file_location, created_date, status) \n            VALUES('extra_column_sales_data_2023_11_01.csv', '/home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv', '2024-04-05 13.49.20', 'A')\n            "]
2024-04-05 13:49:20,169 - INFO - *************************** Connecting to MySql server ***************************
2024-04-05 13:49:20,170 - INFO - *************************** MySql connected Successfully ***************************
2024-04-05 13:49:20,199 - INFO - ********************************** Staging table updated successfully ***************************************
2024-04-05 13:49:20,199 - INFO - **************************** Fixing extra columns coming from source *****************************
2024-04-05 13:49:20,290 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-04-05 13:49:20,846 - INFO - Extra columns present at source. Extra columns: []
2024-04-05 13:49:20,933 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-04-05 13:49:21,224 - INFO - Extra columns present at source. Extra columns: []
2024-04-05 13:49:21,278 - INFO - data: /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-04-05 13:49:21,612 - INFO - Extra columns present at source. Extra columns: ['payment_mode']
2024-04-05 13:49:21,666 - INFO - processed /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv and added additional columns
2024-04-05 13:49:21,675 - INFO - ****************** Final dataframe from source which will be going for processing *****************
2024-04-05 13:49:23,398 - INFO - **************** Loading customer table into customer_table_df *********************
2024-04-05 13:49:24,025 - INFO - **************** Loading product table into product_table_df *********************
2024-04-05 13:49:24,073 - INFO - **************** Loading product staging table into product_staging_table_df *********************
2024-04-05 13:49:24,111 - INFO - **************** Loading sales team table into sales_team_table_df *********************
2024-04-05 13:49:24,154 - INFO - **************** Loading store table into store_table_df *********************
2024-04-05 13:49:24,189 - INFO - Joining the final_df_to_process with customer_table_df 
2024-04-05 13:49:24,252 - INFO - Joining the s3_customer_df_join with store_table_df 
2024-04-05 13:49:24,297 - INFO - Joining the s3_customer_store_df_join with sales_team_table_df 
2024-04-05 13:49:24,409 - INFO - *********************** Final enriched data *****************************
2024-04-05 13:49:27,042 - INFO - ********************** Write the data into customer data mart ******************************
2024-04-05 13:49:27,075 - INFO - ************** Final Data for Customer Data Mart ********************
2024-04-05 13:49:30,752 - INFO - *************** customer data written to local disk at /home/hdoop/projects/project-1/spark_data/customer_data_mart/ **********
2024-04-05 13:49:30,752 - INFO - ************ Data Movement from local to S3 for customer data mart **************
2024-04-05 13:49:31,231 - INFO - Data Successfully uploaded in customer_data_mart data mart 
2024-04-05 13:49:31,231 - INFO - **************** Write the data into sales team data mart *******************
2024-04-05 13:49:31,429 - INFO - ********************** Final Data for Sales Team Data Mart ************************
2024-04-05 13:49:34,044 - INFO - ********** sales team data written to local disk at /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/ **********
2024-04-05 13:49:34,519 - INFO - Data Successfully uploaded in sales_data_mart data mart 
2024-04-05 13:49:42,084 - INFO - ******************* Calculating customer every month purchased amount **********************
2024-04-05 13:49:45,518 - INFO - Data successfully written into customers_data_mart table 
2024-04-05 13:49:45,518 - INFO - ******************* Calculation of customer mart done and written into the tables **********************
2024-04-05 13:49:45,518 - INFO - Calculating sales monthly billed amount
2024-04-05 13:49:47,381 - INFO - Data successfully written into sales_team_data_mart table 
2024-04-05 13:49:47,381 - INFO - Calculation of sales mart done and written into the table: sales_team_data_mart
2024-04-05 13:49:47,936 - INFO - Data Moved successfully from sales_data/ to sales_data_processed/
2024-04-05 13:49:47,937 - INFO - ******************* Deleting sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-04-05 13:49:47,937 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data_2024-01-03.csv
2024-04-05 13:49:47,938 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/file_from_s3/sales_data.csv
2024-04-05 13:49:47,938 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/file_from_s3/extra_column_sales_data_2023_11_01.csv
2024-04-05 13:49:47,938 - INFO - ******************* Deleted sales data from /home/hdoop/projects/project-1/spark_data/file_from_s3/ *******************
2024-04-05 13:49:47,938 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-04-05 13:49:47,938 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/.part-00000-5a5d2fa6-6b8a-42e0-89b0-3275146730d0-c000.snappy.parquet.crc
2024-04-05 13:49:47,939 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/_SUCCESS
2024-04-05 13:49:47,939 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/._SUCCESS.crc
2024-04-05 13:49:47,939 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/customer_data_mart/part-00000-5a5d2fa6-6b8a-42e0-89b0-3275146730d0-c000.snappy.parquet
2024-04-05 13:49:47,939 - INFO - ******************* Deleting customer data mart files from locals *******************
2024-04-05 13:49:47,940 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-04-05 13:49:47,940 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/part-00000-3da1d77a-4835-401b-9d40-eee13f1fe480-c000.snappy.parquet
2024-04-05 13:49:47,940 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/_SUCCESS
2024-04-05 13:49:47,941 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/.part-00000-3da1d77a-4835-401b-9d40-eee13f1fe480-c000.snappy.parquet.crc
2024-04-05 13:49:47,941 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_team_data_mart/._SUCCESS.crc
2024-04-05 13:49:47,941 - INFO - ******************* Deleting sales team data mart files from locals *******************
2024-04-05 13:49:47,941 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-04-05 13:49:47,943 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-05
2024-04-05 13:49:47,945 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-06
2024-04-05 13:49:47,946 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-07
2024-04-05 13:49:47,947 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-03
2024-04-05 13:49:47,948 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_partition_data/_SUCCESS
2024-04-05 13:49:47,949 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-11
2024-04-05 13:49:47,950 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-08
2024-04-05 13:49:47,951 - INFO - Deleted folder: /home/hdoop/projects/project-1/spark_data/sales_partition_data/sales_month=2023-04
2024-04-05 13:49:47,952 - INFO - Deleted file: /home/hdoop/projects/project-1/spark_data/sales_partition_data/._SUCCESS.crc
2024-04-05 13:49:47,952 - INFO - ******************* Deleting sales team data mart partitioned files from locals *******************
2024-04-05 13:49:47,952 - INFO - Update statement created for staging table: ["UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-04-05 13.49.20' WHERE file_name = 'sales_data_2024-01-03.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-04-05 13.49.20' WHERE file_name = 'sales_data.csv'", "UPDATE manish_spark_de.product_staging_table SET status = 'I', updated_date = '2024-04-05 13.49.20' WHERE file_name = 'extra_column_sales_data_2023_11_01.csv'"]
2024-04-05 13:49:47,952 - INFO - **************** Connecting to MySQL server *******************
2024-04-05 13:49:47,953 - INFO - *********** MySQL connected successfully **************
